defaults:
  - hydra: default
  - _self_

hydra:
  searchpath:
    - file://conf/

#train_file: ../research.data/LogiQA2.0/logiqa/DATA/LOGIQA/train.txt
#dev_file: ../research.data/LogiQA2.0/logiqa/DATA/LOGIQA/train.txt
#test_file: ../research.data/LogiQA2.0/logiqa/DATA/LOGIQA/train.txt
test_file: ../data/LogiQA2.0/logiqa/DATA/LOGIQA/train.txt

data_dir: experiments/llama2.7b.chat.logiqav2.gpt35turbo-dpo-sft.H100.w2.v2.0/checkpoint-1600
port: 6000
dataset_split_id: 0

output_file: ${data_dir}/logiqav2-train.full.qa.react.v1.0.0shot.inter.ver2.0.rs0.2.r0.6.split-${dataset_split_id}.sample3.json
flush_file: ${output_file}l

# Data loading
read_tensor:
  _target_: data.logiqav2.ComposePromptGenerator
  read_func:
    _target_: data.logiqav2.SubResponseMergeReader
#    inter_states_file: ${data_dir}/logiqav2-train.full.qa.react.v1.0.1shot.sample20.clean_inter_ver0.0.${dataset_split_id}-of-10.json
    inter_states_file: ${data_dir}/logiqav2-train.full.qa.react.v1.0.1shot.sample20.clean_inter_ver2.0.rs0.2.r0.6.${dataset_split_id}-of-20.json
    flat_options: True
  template_id: 9
  instruction:
    _target_: data.prompts.logiqav2.react.prompts.get_prompt
    prompt_name: react_v2
  compose_keys: [ "context", "question", "option_list", "response" ]
  max_data_num: -1
  api_based: False
  service_based: True
  service_processor:
    _target_: data.vllm.VLLMRequestGenerator
    api_url: http://0.0.0.0:${port}/v1/completions
    max_tokens: 2048
    model: llama-2-7b-sft-v4.1-cp1600
    n: 3
    temperature: 0.7
    top_p: 0.8
    stop: [ "</s>", "\n\n\n\n" ]
  flush_file: ${flush_file}

# Dataloader
num_workers: 32
prefetch_factor: 2

output_dir:

post_process:
  _target_: post_processors.openai_api_callback.OpenAICallBack
  output_file: ${output_file}
  answer_clean:
    _target_: post_processors.openai_api_callback.ReActSeparatorClean
#    prompt: "few-shot"
#    separator: "Finish"
#    separate_idx: 1


# Training hyper-parameters
per_gpu_train_batch_size: 1
per_gpu_eval_batch_size: 1

ddp_eval: False
no_cuda: False
seed: 42
local_rank: -1

# Temporary variables
n_gpu: 1
device:
train_batch_size:
eval_batch_size:
world_size:
