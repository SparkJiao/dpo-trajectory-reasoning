defaults:
  - hydra: default
  - post_process: openai_react
  - api/vllm/vllm_params@sampling_params: sampling_param_greedy
  - _self_

hydra:
  searchpath:
    - file://conf/

train_file: ../research.data/FOLIO/data/v0.0/folio-train.jsonl
dev_file: ../research.data/FOLIO/data/v0.0/folio-validation.jsonl
test_file: ../research.data/FOLIO/data/v0.0/folio-validation.jsonl

save_best: False
exp_name:
exp_notes:
output_dir: experiments/${exp_name}

step: 800
eval_sub_path: checkpoint-${step}

read_tensor:
  _target_: data.logiqav2.ComposePromptGenerator
  read_func:
    _target_: data.folio.FOLIO2QAReader
  instruction:
    _target_: data.prompts.logiqav2.react.prompts.get_prompt
    prompt_name: react_v2
  few_shot_prompt:
  compose_keys: [ "context", "question", "option_list" ]
  max_data_num: -1
  api_based: False
  template_id: "Context:\n{}\n\nQuestion:\n{}\n\nOptions:\n{}\n\nThought 1: "  # In version v2.1, we change the template to: ```xxx\n\nThought 1: ```
  service_based: False
  service_processor:

output_file: ${output_dir}/${eval_sub_path}/folio.dev.qa.react.v1.0.0shot.json
flush_file: ${output_file}l

post_process:
  answer_clean:
    _target_: post_processors.openai_api_callback.ReActSeparatorClean
    regrex: "A|B"

# Dataloader
num_workers: 48
prefetch_factor: 2

ddp_eval: False
no_cuda: False
seed: 42
local_rank: -1

# Temporary variables
fp16: True
fp16_bfloat16: True
n_gpu: 1
device:
train_batch_size:
eval_batch_size:
world_size:
