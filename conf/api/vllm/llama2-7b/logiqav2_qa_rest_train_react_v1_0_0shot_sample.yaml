defaults:
  - hydra: default
  - reader/logiqav2@read_tensor: react_service_0shot_v1_0
  - post_process: openai_react
#  - api/vllm/vllm_params@sampling_params: sampling_param_greedy
  - _self_

hydra:
  searchpath:
    - file://conf/

train_file: ../research.data/LogiQA2.0/logiqa/DATA/LOGIQA/train.txt
dev_file: ../research.data/LogiQA2.0/logiqa/DATA/LOGIQA/train.txt
test_file: ../research.data/LogiQA2.0/logiqa/DATA/LOGIQA/train.txt

step: 2000
eval_sub_path: checkpoint-${step}

n: 10
split_size: 4
split_id: 0


# Data loading
read_tensor:
  template_id: "Context:\n{}\n\nQuestion:\n{}\n\nOptions:\n{}\n\nThought 1: "
  split_size: ${split_size}
  split_id: ${split_id}
  service_based: False
  service_processor:

sampling_params:
  _target_: vllm.SamplingParams
  n: ${n}
  temperature: 1.0
  top_p: 0.8
  stop: [ "</s>", "\n\n\n\n" ]
  max_tokens: 2048

save_best: False
output_dir: experiments/llama2.7b.chat.logiqav2.70b-distil.self-sft.A40.w8.v1.0

suffix: ${n}.tem${sampling_params.temperature}.p${sampling_params.top_p}.s${split_id}-of-${split_size}
output_file: ${output_dir}/${eval_sub_path}/logiqav2-train.full.qa.react.v1.0.0shot.${suffix}.json
flush_file: ${output_file}l

# Dataloader
num_workers: 96
prefetch_factor: 2


# Training hyper-parameters
per_gpu_train_batch_size: 1
per_gpu_eval_batch_size: 1

ddp_eval: False
no_cuda: False
seed: 42
local_rank: -1

# Temporary variables
fp16: True
fp16_bfloat16: True
n_gpu: 1
device:
train_batch_size:
eval_batch_size:
world_size:
