defaults:
  - hydra: default
  - reader/reclor@read_tensor: react_service_0shot_v1_0
  - post_process: openai_react
  - api/vllm/vllm_params@sampling_params: sampling_param_sample
  - _self_

hydra:
  searchpath:
    - file://conf/

train_file: ../research.data/reclor_data/train.json
dev_file: ../research.data/reclor_data/val.json
test_file: ${dev_file}

save_best: False
exp_name:
exp_notes:
output_dir: experiments/${exp_name}

step:
eval_sub_path: checkpoint-${step}

sampling_params:
  max_tokens: 2048
gpu_memory_utilization: 0.95

read_tensor:
  template_id: "Context:\n{}\n\nQuestion:\n{}\n\nOptions:\n{}\n\nThought 1: "  # In version v2.1, we change the template to: ```xxx\n\nThought 1: ```
  service_based: False
  service_processor:

output_file: ${output_dir}/${eval_sub_path}/reclor.react.dev.n${sampling_params.n}.tem${sampling_params.temperature}.0shot.v1.1.json
flush_file: ${output_file}l

# Dataloader
num_workers: 32
prefetch_factor:

# Training hyper-parameters
per_gpu_train_batch_size: 1
per_gpu_eval_batch_size: 1

ddp_eval: False
no_cuda: False
seed: 42
local_rank: -1

# Temporary variables
fp16: True
fp16_bfloat16: True
n_gpu: 1
device:
train_batch_size:
eval_batch_size:
world_size:

# CUDA_VISIBLE_DEVICES=0 run15 python vllm_inference.py  -cp conf/api/vllm/mistral/reclor/ -cn train_react_1shot_sample5_split_v1_0 read_tensor.split_size=4 read_tensor.split_id=0
