defaults:
  - hydra: default
  - reader/reclor@read_tensor: react_service_0shot_v1_0
  - post_process: openai_react
#  - api/vllm/vllm_params@sampling_params: sampling_param_sample
  - _self_

hydra:
  searchpath:
    - file://conf/

train_file: ../research.data/reclor_data/train.json
dev_file: ../research.data/reclor_data/val.json
test_file: ${train_file}
#test_file: ../research.data/reclor_data/val.json

save_best: False
exp_name:
exp_notes:
#output_dir: experiments/llama2.7b.chat.mixtral.dpo-sft.A100.40.w8.v1.0/checkpoint-1200/
#output_dir: experiments/llama2.7b.chat.reclor.gpt35turbo1106.dpo-sft.A100.w2.v1.0/checkpoint-2400/  # @2024/01/10
#output_dir: experiments/llama2.7b.chat.logiqav2.70b-distil.step.dpo.fix_hack.H100.w4.v1.0.th.s43/checkpoint-2000/  # @2024/01/11
output_dir: experiments/llama2.7b.chat.reclor.gpt35turbo1106.dpo-sft.H100.w4.v2.0/checkpoint-1200 # @2024/01/12
eval_sub_path:

split_size: 2
split_id: 0
port: 6000

#sampling_params:
#  max_tokens: 3072
gpu_memory_utilization: 0.95

read_tensor:
  split_size: ${split_size}
  split_id: ${split_id}
  service_based: True
  service_processor:
    _target_: data.vllm.VLLMRequestGenerator
    api_url: http://0.0.0.0:${port}/v1/completions
    max_tokens: 3072
    model: llama2-7b-reclor-distil
#    model: llama2-7b-logiqa-step-dpo
    stop: [ "</s>", "\n\n\n\n", "Context:\n" ]
#    n: 20
    n: 10
#    temperature: 2.0
#    temperature: 1.0
    temperature: 0.7
  flush_file: ${flush_file}

#output_file: ${output_dir}/reclor.react.train.0shot.sample${read_tensor.service_processor.n}.tem${read_tensor.service_processor.temperature}.v1.0.json
#output_file: ${output_dir}/reclor.react.dev.0shot.sample${read_tensor.service_processor.n}.tem${read_tensor.service_processor.temperature}.v1.0.json  # @2024/01/10
#output_file: ${output_dir}/reclor.react.train.0shot.sample${read_tensor.service_processor.n}.tem${read_tensor.service_processor.temperature}.${split_id}-of-${split_size}v1.0.json  # @2024/01/10
output_file: ${output_dir}/reclor.react.train.0shot.sample${read_tensor.service_processor.n}.tem${read_tensor.service_processor.temperature}.${split_id}-of-${split_size}v1.0.json  # @2024/01/11
flush_file: ${output_file}l

# Dataloader
num_workers: 16
prefetch_factor:

# Training hyper-parameters
per_gpu_train_batch_size: 1
per_gpu_eval_batch_size: 1

ddp_eval: False
no_cuda: False
seed: 42
local_rank: -1

# Temporary variables
fp16: True
fp16_bfloat16: True
n_gpu: 1
device:
train_batch_size:
eval_batch_size:
world_size:

# CUDA_VISIBLE_DEVICES=0 run15 python vllm_inference.py  -cp conf/api/vllm/mistral/reclor/ -cn train_react_1shot_sample5_split_v1_0 read_tensor.split_size=4 read_tensor.split_id=0
